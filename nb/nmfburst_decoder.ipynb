{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmfburst_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcDMOhDIiCygaozejw6F88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changhoonhahn/provabgs/blob/main/nb/nmfburst_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azXSJtoGUK4J"
      },
      "source": [
        "# decorder for `nmfburst` SPS model\n",
        "Instead of using the PCA encoding a training a neural net to predict PCA coefficients, I'm going to try to train a decoder directly from the (theta, SED) data set. \n",
        "\n",
        "notebook has code lifted from: \n",
        "- https://github.com/stephenportillo/SDSS-VAE/blob/master/trainVAE.py\n",
        "- https://github.com/stephenportillo/SDSS-VAE/blob/master/InfoVAE.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwRbs0kyUHQ5",
        "outputId": "6c27e144-ffad-482e-a7b4-9fdc7d290e99"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1HlSzJjVHPv",
        "outputId": "a08e902d-cc33-4de1-dd45-fdd26b2e1463"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/provabgs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/provabgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6MCG2AYVj86"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmz6NBcwZt3k"
      },
      "source": [
        "theta = np.load(\"fsps.nmfburst.theta.test.npy\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kga3a9d2aDRW"
      },
      "source": [
        "# whiten the spectra\n",
        "mu_lnspec = np.mean(np.load('fsps.nmfburst.lnspectrum.test.npy'), axis=0)\n",
        "sig_lnspec = np.std(np.load('fsps.nmfburst.lnspectrum.test.npy'), axis=0)\n",
        "\n",
        "lnspec_white = (np.load('fsps.nmfburst.lnspectrum.test.npy') - mu_lnspec)/sig_lnspec"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LDtth7ga4EO",
        "outputId": "eec9c67c-e840-47d1-d824-92837b1b5f2d"
      },
      "source": [
        "n_theta = theta.shape[1]\n",
        "n_lnspec = lnspec_white.shape[1]\n",
        "print('n theta = %i' % n_theta)\n",
        "print('n ln(spec) = %i' % n_lnspec)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n theta = 12\n",
            "n ln(spec) = 4469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3ru-f0QaZ4h",
        "outputId": "1fac6c7c-03d1-4111-c338-469953b76813"
      },
      "source": [
        "Ntrain = int(float(theta.shape[0]) * 0.9)\n",
        "Ntest = theta.shape[0] - Ntrain\n",
        "print('Ntrain = %i' % Ntrain)\n",
        "print('Ntest = %i' % Ntest)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ntrain = 90000\n",
            "Ntest = 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wtiQicGZjB1"
      },
      "source": [
        "batch_size=64\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(torch.tensor(theta[:Ntrain], dtype=torch.float32), torch.tensor(lnspec_white[:Ntrain], dtype=torch.float32)),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAw-cQ1sVh03"
      },
      "source": [
        "class Decoder(nn.Module): \n",
        "    def __init__(self, nfeat=1000, ncode=5, nhidden=128, nhidden2=35, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.ncode = int(ncode)\n",
        "        \n",
        "        self.decd = nn.Linear(ncode, nhidden2)\n",
        "        self.d3 = nn.Dropout(p=dropout)\n",
        "        self.dec2 = nn.Linear(nhidden2, nhidden)\n",
        "        self.d4 = nn.Dropout(p=dropout)\n",
        "        self.outp = nn.Linear(nhidden, nfeat)\n",
        "        \n",
        "    def decode(self, x):\n",
        "        x = self.d3(F.leaky_relu(self.decd(x)))\n",
        "        x = self.d4(F.leaky_relu(self.dec2(x)))\n",
        "        x = self.outp(x)\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "    \n",
        "    def loss(self, x, y):\n",
        "        recon_y = self.forward(x)\n",
        "        MSE = torch.sum(0.5 * (y - recon_y).pow(2))\n",
        "        return MSE"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1QNfEfpV-rH"
      },
      "source": [
        "def train(): #model, optimizer, epoch, min_valid_loss, badepochs\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        tt, lns = data\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(tt, lns)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    return train_loss \n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, precision=1e-3, patience=10):\n",
        "        self.precision = precision\n",
        "        self.patience = patience\n",
        "        self.badepochs = 0\n",
        "        self.min_valid_loss = float('inf')\n",
        "        \n",
        "    def step(self, valid_loss):\n",
        "        if valid_loss < self.min_valid_loss*(1-self.precision):\n",
        "            self.badepochs = 0\n",
        "            self.min_valid_loss = valid_loss\n",
        "        else:\n",
        "            self.badepochs += 1\n",
        "        return not (self.badepochs == self.patience)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKBo9AqAV-k7",
        "outputId": "4c27eeb2-a19f-44d8-fe80-233a5d905afc"
      },
      "source": [
        "epochs = 200\n",
        "log_interval = 10\n",
        "n_config = 1\n",
        "\n",
        "for config in range(n_config):\n",
        "    dropout = 0. #0.9*np.random.uniform()\n",
        "    dfac = 1./(1.-dropout)\n",
        "    nhidden = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*n_theta+1), np.log(dfac*2*n_lnspec)))))\n",
        "    nhidden2 = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*n_theta+1), np.log(nhidden)))))\n",
        "    print('config %i, dropout = %0.2f; 2 hidden layers with %i, %i nodes' % (config, dropout, nhidden, nhidden2))\n",
        "    model = Decoder(nfeat=n_lnspec, nhidden=nhidden, nhidden2=nhidden2, ncode=n_theta, dropout=dropout)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5)\n",
        "    stopper = EarlyStopper(patience=10)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train()\n",
        "        print('====> Epoch: {} TRAINING Loss: {:.2e}'.format(epoch, train_loss))\n",
        "        #if epoch % log_interval == 0:\n",
        "        #    print('====> Epoch: {} TRAINING Loss: {:.2e}'.format(epoch, train_loss))\n",
        "\n",
        "        scheduler.step(train_loss)\n",
        "        if (not stopper.step(train_loss)) or (epoch == epochs):\n",
        "            print('Stopping')\n",
        "            print('====> Epoch: {} TRAINING Loss: {:.2e}'.format(epoch, train_loss))\n",
        "            #torch.save(model, tag+'/%04i.pth' % config)\n",
        "            break \n",
        "        torch.save(model, 'decoder.pth')\n",
        "#np.savez(tag+'/metrics.npz', MSE=mdl_MSE, KLD=mdl_KLD, MMD=mdl_MMD)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config 0, dropout = 0.00; 2 hidden layers with 508, 31 nodes\n",
            "====> Epoch: 1 TRAINING Loss: 1.57e+02\n",
            "====> Epoch: 2 TRAINING Loss: 5.99e+01\n",
            "====> Epoch: 3 TRAINING Loss: 4.13e+01\n",
            "====> Epoch: 4 TRAINING Loss: 3.08e+01\n",
            "====> Epoch: 5 TRAINING Loss: 2.58e+01\n",
            "====> Epoch: 6 TRAINING Loss: 2.33e+01\n",
            "====> Epoch: 7 TRAINING Loss: 2.08e+01\n",
            "====> Epoch: 8 TRAINING Loss: 1.97e+01\n",
            "====> Epoch: 9 TRAINING Loss: 1.80e+01\n",
            "====> Epoch: 10 TRAINING Loss: 1.71e+01\n",
            "====> Epoch: 11 TRAINING Loss: 1.63e+01\n",
            "====> Epoch: 12 TRAINING Loss: 1.56e+01\n",
            "====> Epoch: 13 TRAINING Loss: 1.49e+01\n",
            "====> Epoch: 14 TRAINING Loss: 1.46e+01\n",
            "====> Epoch: 15 TRAINING Loss: 1.39e+01\n",
            "====> Epoch: 16 TRAINING Loss: 1.36e+01\n",
            "====> Epoch: 17 TRAINING Loss: 1.31e+01\n",
            "====> Epoch: 18 TRAINING Loss: 1.25e+01\n",
            "====> Epoch: 19 TRAINING Loss: 1.19e+01\n",
            "====> Epoch: 20 TRAINING Loss: 1.16e+01\n",
            "====> Epoch: 21 TRAINING Loss: 1.16e+01\n",
            "====> Epoch: 22 TRAINING Loss: 1.08e+01\n",
            "====> Epoch: 23 TRAINING Loss: 1.07e+01\n",
            "====> Epoch: 24 TRAINING Loss: 1.02e+01\n",
            "====> Epoch: 25 TRAINING Loss: 9.89e+00\n",
            "====> Epoch: 26 TRAINING Loss: 9.94e+00\n",
            "====> Epoch: 27 TRAINING Loss: 9.30e+00\n",
            "====> Epoch: 28 TRAINING Loss: 9.92e+00\n",
            "====> Epoch: 29 TRAINING Loss: 8.99e+00\n",
            "====> Epoch: 30 TRAINING Loss: 8.77e+00\n",
            "====> Epoch: 31 TRAINING Loss: 8.81e+00\n",
            "====> Epoch: 32 TRAINING Loss: 8.68e+00\n",
            "====> Epoch: 33 TRAINING Loss: 8.30e+00\n",
            "====> Epoch: 34 TRAINING Loss: 8.34e+00\n",
            "====> Epoch: 35 TRAINING Loss: 7.96e+00\n",
            "====> Epoch: 36 TRAINING Loss: 8.34e+00\n",
            "====> Epoch: 37 TRAINING Loss: 8.02e+00\n",
            "====> Epoch: 38 TRAINING Loss: 8.08e+00\n",
            "====> Epoch: 39 TRAINING Loss: 7.61e+00\n",
            "====> Epoch: 40 TRAINING Loss: 7.79e+00\n",
            "====> Epoch: 41 TRAINING Loss: 7.81e+00\n",
            "====> Epoch: 42 TRAINING Loss: 7.40e+00\n",
            "====> Epoch: 43 TRAINING Loss: 7.63e+00\n",
            "====> Epoch: 44 TRAINING Loss: 7.36e+00\n",
            "====> Epoch: 45 TRAINING Loss: 7.37e+00\n",
            "====> Epoch: 46 TRAINING Loss: 7.10e+00\n",
            "====> Epoch: 47 TRAINING Loss: 7.24e+00\n",
            "====> Epoch: 48 TRAINING Loss: 6.87e+00\n",
            "====> Epoch: 49 TRAINING Loss: 6.71e+00\n",
            "====> Epoch: 50 TRAINING Loss: 6.77e+00\n",
            "====> Epoch: 51 TRAINING Loss: 6.70e+00\n",
            "====> Epoch: 52 TRAINING Loss: 6.61e+00\n",
            "====> Epoch: 53 TRAINING Loss: 6.64e+00\n",
            "====> Epoch: 54 TRAINING Loss: 6.40e+00\n",
            "====> Epoch: 55 TRAINING Loss: 6.35e+00\n",
            "====> Epoch: 56 TRAINING Loss: 6.37e+00\n",
            "====> Epoch: 57 TRAINING Loss: 6.33e+00\n",
            "====> Epoch: 58 TRAINING Loss: 6.54e+00\n",
            "====> Epoch: 59 TRAINING Loss: 6.30e+00\n",
            "====> Epoch: 60 TRAINING Loss: 6.29e+00\n",
            "====> Epoch: 61 TRAINING Loss: 6.19e+00\n",
            "====> Epoch: 62 TRAINING Loss: 6.18e+00\n",
            "====> Epoch: 63 TRAINING Loss: 5.92e+00\n",
            "====> Epoch: 64 TRAINING Loss: 6.12e+00\n",
            "====> Epoch: 65 TRAINING Loss: 5.84e+00\n",
            "====> Epoch: 66 TRAINING Loss: 5.88e+00\n",
            "====> Epoch: 67 TRAINING Loss: 5.86e+00\n",
            "====> Epoch: 68 TRAINING Loss: 5.98e+00\n",
            "====> Epoch: 69 TRAINING Loss: 5.79e+00\n",
            "====> Epoch: 70 TRAINING Loss: 5.79e+00\n",
            "====> Epoch: 71 TRAINING Loss: 5.88e+00\n",
            "====> Epoch: 72 TRAINING Loss: 5.75e+00\n",
            "====> Epoch: 73 TRAINING Loss: 5.74e+00\n",
            "====> Epoch: 74 TRAINING Loss: 5.82e+00\n",
            "====> Epoch: 75 TRAINING Loss: 5.54e+00\n",
            "====> Epoch: 76 TRAINING Loss: 5.51e+00\n",
            "====> Epoch: 77 TRAINING Loss: 5.78e+00\n",
            "====> Epoch: 78 TRAINING Loss: 5.50e+00\n",
            "====> Epoch: 79 TRAINING Loss: 5.46e+00\n",
            "====> Epoch: 80 TRAINING Loss: 5.45e+00\n",
            "====> Epoch: 81 TRAINING Loss: 5.24e+00\n",
            "====> Epoch: 82 TRAINING Loss: 5.33e+00\n",
            "====> Epoch: 83 TRAINING Loss: 5.41e+00\n",
            "====> Epoch: 84 TRAINING Loss: 5.28e+00\n",
            "====> Epoch: 85 TRAINING Loss: 5.25e+00\n",
            "====> Epoch: 86 TRAINING Loss: 5.25e+00\n",
            "====> Epoch: 87 TRAINING Loss: 5.17e+00\n",
            "====> Epoch: 88 TRAINING Loss: 5.21e+00\n",
            "====> Epoch: 89 TRAINING Loss: 5.16e+00\n",
            "====> Epoch: 90 TRAINING Loss: 5.13e+00\n",
            "====> Epoch: 91 TRAINING Loss: 5.17e+00\n",
            "====> Epoch: 92 TRAINING Loss: 5.16e+00\n",
            "====> Epoch: 93 TRAINING Loss: 5.09e+00\n",
            "====> Epoch: 94 TRAINING Loss: 5.00e+00\n",
            "====> Epoch: 95 TRAINING Loss: 5.00e+00\n",
            "====> Epoch: 96 TRAINING Loss: 5.09e+00\n",
            "====> Epoch: 97 TRAINING Loss: 5.09e+00\n",
            "====> Epoch: 98 TRAINING Loss: 5.01e+00\n",
            "====> Epoch: 99 TRAINING Loss: 4.90e+00\n",
            "====> Epoch: 100 TRAINING Loss: 5.15e+00\n",
            "====> Epoch: 101 TRAINING Loss: 4.86e+00\n",
            "====> Epoch: 102 TRAINING Loss: 4.70e+00\n",
            "====> Epoch: 103 TRAINING Loss: 5.00e+00\n",
            "====> Epoch: 104 TRAINING Loss: 4.92e+00\n",
            "====> Epoch: 105 TRAINING Loss: 4.79e+00\n",
            "====> Epoch: 106 TRAINING Loss: 4.78e+00\n",
            "====> Epoch: 107 TRAINING Loss: 4.74e+00\n",
            "====> Epoch: 108 TRAINING Loss: 4.73e+00\n",
            "Epoch   108: reducing learning rate of group 0 to 1.0000e-04.\n",
            "====> Epoch: 109 TRAINING Loss: 3.00e+00\n",
            "====> Epoch: 110 TRAINING Loss: 2.81e+00\n",
            "====> Epoch: 111 TRAINING Loss: 2.76e+00\n",
            "====> Epoch: 112 TRAINING Loss: 2.75e+00\n",
            "====> Epoch: 113 TRAINING Loss: 2.74e+00\n",
            "====> Epoch: 114 TRAINING Loss: 2.72e+00\n",
            "====> Epoch: 115 TRAINING Loss: 2.72e+00\n",
            "====> Epoch: 116 TRAINING Loss: 2.67e+00\n",
            "====> Epoch: 117 TRAINING Loss: 2.69e+00\n",
            "====> Epoch: 118 TRAINING Loss: 2.67e+00\n",
            "====> Epoch: 119 TRAINING Loss: 2.66e+00\n",
            "====> Epoch: 120 TRAINING Loss: 2.63e+00\n",
            "====> Epoch: 121 TRAINING Loss: 2.63e+00\n",
            "====> Epoch: 122 TRAINING Loss: 2.62e+00\n",
            "====> Epoch: 123 TRAINING Loss: 2.61e+00\n",
            "====> Epoch: 124 TRAINING Loss: 2.59e+00\n",
            "====> Epoch: 125 TRAINING Loss: 2.59e+00\n",
            "====> Epoch: 126 TRAINING Loss: 2.59e+00\n",
            "====> Epoch: 127 TRAINING Loss: 2.58e+00\n",
            "====> Epoch: 128 TRAINING Loss: 2.58e+00\n",
            "====> Epoch: 129 TRAINING Loss: 2.56e+00\n",
            "====> Epoch: 130 TRAINING Loss: 2.56e+00\n",
            "====> Epoch: 131 TRAINING Loss: 2.53e+00\n",
            "====> Epoch: 132 TRAINING Loss: 2.53e+00\n",
            "====> Epoch: 133 TRAINING Loss: 2.54e+00\n",
            "====> Epoch: 134 TRAINING Loss: 2.52e+00\n",
            "====> Epoch: 135 TRAINING Loss: 2.51e+00\n",
            "====> Epoch: 136 TRAINING Loss: 2.51e+00\n",
            "====> Epoch: 137 TRAINING Loss: 2.48e+00\n",
            "====> Epoch: 138 TRAINING Loss: 2.48e+00\n",
            "====> Epoch: 139 TRAINING Loss: 2.48e+00\n",
            "====> Epoch: 141 TRAINING Loss: 2.47e+00\n",
            "====> Epoch: 142 TRAINING Loss: 2.48e+00\n",
            "====> Epoch: 143 TRAINING Loss: 2.45e+00\n",
            "====> Epoch: 144 TRAINING Loss: 2.45e+00\n",
            "====> Epoch: 145 TRAINING Loss: 2.44e+00\n",
            "====> Epoch: 146 TRAINING Loss: 2.44e+00\n",
            "====> Epoch: 147 TRAINING Loss: 2.43e+00\n",
            "====> Epoch: 148 TRAINING Loss: 2.42e+00\n",
            "====> Epoch: 149 TRAINING Loss: 2.41e+00\n",
            "====> Epoch: 150 TRAINING Loss: 2.43e+00\n",
            "====> Epoch: 151 TRAINING Loss: 2.41e+00\n",
            "====> Epoch: 152 TRAINING Loss: 2.40e+00\n",
            "====> Epoch: 153 TRAINING Loss: 2.39e+00\n",
            "====> Epoch: 154 TRAINING Loss: 2.39e+00\n",
            "====> Epoch: 155 TRAINING Loss: 2.38e+00\n",
            "====> Epoch: 156 TRAINING Loss: 2.38e+00\n",
            "====> Epoch: 157 TRAINING Loss: 2.40e+00\n",
            "====> Epoch: 158 TRAINING Loss: 2.35e+00\n",
            "====> Epoch: 159 TRAINING Loss: 2.35e+00\n",
            "====> Epoch: 160 TRAINING Loss: 2.35e+00\n",
            "====> Epoch: 161 TRAINING Loss: 2.37e+00\n",
            "====> Epoch: 162 TRAINING Loss: 2.34e+00\n",
            "====> Epoch: 163 TRAINING Loss: 2.35e+00\n",
            "====> Epoch: 164 TRAINING Loss: 2.32e+00\n",
            "====> Epoch: 165 TRAINING Loss: 2.33e+00\n",
            "====> Epoch: 166 TRAINING Loss: 2.32e+00\n",
            "====> Epoch: 167 TRAINING Loss: 2.33e+00\n",
            "====> Epoch: 168 TRAINING Loss: 2.34e+00\n",
            "====> Epoch: 169 TRAINING Loss: 2.30e+00\n",
            "====> Epoch: 170 TRAINING Loss: 2.30e+00\n",
            "====> Epoch: 171 TRAINING Loss: 2.31e+00\n",
            "====> Epoch: 172 TRAINING Loss: 2.31e+00\n",
            "====> Epoch: 173 TRAINING Loss: 2.28e+00\n",
            "====> Epoch: 174 TRAINING Loss: 2.29e+00\n",
            "====> Epoch: 175 TRAINING Loss: 2.30e+00\n",
            "====> Epoch: 176 TRAINING Loss: 2.29e+00\n",
            "====> Epoch: 177 TRAINING Loss: 2.27e+00\n",
            "====> Epoch: 178 TRAINING Loss: 2.28e+00\n",
            "====> Epoch: 179 TRAINING Loss: 2.28e+00\n",
            "====> Epoch: 180 TRAINING Loss: 2.25e+00\n",
            "====> Epoch: 181 TRAINING Loss: 2.27e+00\n",
            "====> Epoch: 182 TRAINING Loss: 2.26e+00\n",
            "====> Epoch: 183 TRAINING Loss: 2.26e+00\n",
            "====> Epoch: 184 TRAINING Loss: 2.23e+00\n",
            "====> Epoch: 185 TRAINING Loss: 2.26e+00\n",
            "====> Epoch: 186 TRAINING Loss: 2.25e+00\n",
            "====> Epoch: 187 TRAINING Loss: 2.24e+00\n",
            "====> Epoch: 188 TRAINING Loss: 2.21e+00\n",
            "====> Epoch: 189 TRAINING Loss: 2.21e+00\n",
            "====> Epoch: 190 TRAINING Loss: 2.21e+00\n",
            "====> Epoch: 191 TRAINING Loss: 2.24e+00\n",
            "====> Epoch: 192 TRAINING Loss: 2.20e+00\n",
            "====> Epoch: 193 TRAINING Loss: 2.22e+00\n",
            "====> Epoch: 194 TRAINING Loss: 2.16e+00\n",
            "====> Epoch: 195 TRAINING Loss: 2.21e+00\n",
            "====> Epoch: 196 TRAINING Loss: 2.20e+00\n",
            "====> Epoch: 197 TRAINING Loss: 2.18e+00\n",
            "====> Epoch: 198 TRAINING Loss: 2.20e+00\n",
            "====> Epoch: 199 TRAINING Loss: 2.18e+00\n",
            "====> Epoch: 200 TRAINING Loss: 2.18e+00\n",
            "Epoch   200: reducing learning rate of group 0 to 1.0000e-05.\n",
            "Stopping\n",
            "====> Epoch: 200 TRAINING Loss: 2.18e+00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBXLhxGtOsb8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}