{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmfburst_decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcDMOhDIiCygaozejw6F88",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/changhoonhahn/provabgs/blob/main/nb/nmfburst_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azXSJtoGUK4J"
      },
      "source": [
        "# decorder for `nmfburst` SPS model\n",
        "Instead of using the PCA encoding a training a neural net to predict PCA coefficients, I'm going to try to train a decoder directly from the (theta, SED) data set. \n",
        "\n",
        "notebook has code lifted from: \n",
        "- https://github.com/stephenportillo/SDSS-VAE/blob/master/trainVAE.py\n",
        "- https://github.com/stephenportillo/SDSS-VAE/blob/master/InfoVAE.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwRbs0kyUHQ5",
        "outputId": "6c27e144-ffad-482e-a7b4-9fdc7d290e99"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1HlSzJjVHPv",
        "outputId": "a08e902d-cc33-4de1-dd45-fdd26b2e1463"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/provabgs"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/provabgs\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6MCG2AYVj86"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.nn import functional as F"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cmz6NBcwZt3k"
      },
      "source": [
        "theta = np.load(\"fsps.nmfburst.theta.test.npy\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kga3a9d2aDRW"
      },
      "source": [
        "# whiten the spectra\n",
        "mu_lnspec = np.mean(np.load('fsps.nmfburst.lnspectrum.test.npy'), axis=0)\n",
        "sig_lnspec = np.std(np.load('fsps.nmfburst.lnspectrum.test.npy'), axis=0)\n",
        "\n",
        "lnspec_white = (np.load('fsps.nmfburst.lnspectrum.test.npy') - mu_lnspec)/sig_lnspec"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LDtth7ga4EO",
        "outputId": "eec9c67c-e840-47d1-d824-92837b1b5f2d"
      },
      "source": [
        "n_theta = theta.shape[1]\n",
        "n_lnspec = lnspec_white.shape[1]\n",
        "print('n theta = %i' % n_theta)\n",
        "print('n ln(spec) = %i' % n_lnspec)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "n theta = 12\n",
            "n ln(spec) = 4469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3ru-f0QaZ4h",
        "outputId": "1fac6c7c-03d1-4111-c338-469953b76813"
      },
      "source": [
        "Ntrain = int(float(theta.shape[0]) * 0.9)\n",
        "Ntest = theta.shape[0] - Ntrain\n",
        "print('Ntrain = %i' % Ntrain)\n",
        "print('Ntest = %i' % Ntest)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ntrain = 90000\n",
            "Ntest = 10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wtiQicGZjB1"
      },
      "source": [
        "batch_size=64\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(torch.tensor(theta[:Ntrain], dtype=torch.float32), torch.tensor(lnspec_white[:Ntrain], dtype=torch.float32)),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAw-cQ1sVh03"
      },
      "source": [
        "class Decoder(nn.Module): \n",
        "    def __init__(self, nfeat=1000, ncode=5, nhidden=128, nhidden2=35, dropout=0.2):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        self.ncode = int(ncode)\n",
        "        \n",
        "        self.decd = nn.Linear(ncode, nhidden2)\n",
        "        self.d3 = nn.Dropout(p=dropout)\n",
        "        self.dec2 = nn.Linear(nhidden2, nhidden)\n",
        "        self.d4 = nn.Dropout(p=dropout)\n",
        "        self.outp = nn.Linear(nhidden, nfeat)\n",
        "        \n",
        "    def decode(self, x):\n",
        "        x = self.d3(F.leaky_relu(self.decd(x)))\n",
        "        x = self.d4(F.leaky_relu(self.dec2(x)))\n",
        "        x = self.outp(x)\n",
        "        return x\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.decode(x)\n",
        "    \n",
        "    def loss(self, x, y):\n",
        "        recon_y = self.forward(x)\n",
        "        MSE = torch.sum(0.5 * (y - recon_y).pow(2))\n",
        "        return MSE"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1QNfEfpV-rH"
      },
      "source": [
        "def train(): #model, optimizer, epoch, min_valid_loss, badepochs\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, data in enumerate(train_loader):\n",
        "        tt, lns = data\n",
        "        optimizer.zero_grad()\n",
        "        loss = model.loss(tt, lns)\n",
        "        loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    return train_loss \n",
        "\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, precision=1e-3, patience=10):\n",
        "        self.precision = precision\n",
        "        self.patience = patience\n",
        "        self.badepochs = 0\n",
        "        self.min_valid_loss = float('inf')\n",
        "        \n",
        "    def step(self, valid_loss):\n",
        "        if valid_loss < self.min_valid_loss*(1-self.precision):\n",
        "            self.badepochs = 0\n",
        "            self.min_valid_loss = valid_loss\n",
        "        else:\n",
        "            self.badepochs += 1\n",
        "        return not (self.badepochs == self.patience)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKBo9AqAV-k7",
        "outputId": "5c4e083f-84f9-45da-f4d7-19528d664e42"
      },
      "source": [
        "epochs = 200\n",
        "log_interval = 10\n",
        "n_config = 1\n",
        "\n",
        "for config in range(n_config):\n",
        "    dropout = 0. #0.9*np.random.uniform()\n",
        "    dfac = 1./(1.-dropout)\n",
        "    nhidden = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*n_theta+1), np.log(dfac*2*n_lnspec)))))\n",
        "    nhidden2 = int(np.ceil(np.exp(np.random.uniform(np.log(dfac*n_theta+1), np.log(nhidden)))))\n",
        "    print('config %i, dropout = %0.2f; 2 hidden layers with %i, %i nodes' % (config, dropout, nhidden, nhidden2))\n",
        "    model = Decoder(nfeat=n_lnspec, nhidden=nhidden, nhidden2=nhidden2, ncode=n_theta, dropout=dropout)\n",
        "    \n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', verbose=True, patience=5)\n",
        "    stopper = EarlyStopper(patience=10)\n",
        "\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train_loss = train()\n",
        "        print('====> Epoch: {} TRAINING Loss: {:.2e}'.format(epoch, train_loss))\n",
        "        #if epoch % log_interval == 0:\n",
        "        #    print('====> Epoch: {} TRAINING Loss: {:.2e}'.format(epoch, train_loss))\n",
        "\n",
        "        scheduler.step(train_loss)\n",
        "        if (not stopper.step(train_loss)) or (epoch == epochs):\n",
        "            print('Stopping')\n",
        "            print('====> Epoch: {} TRAINING Loss: {:.2e}'.format(epoch, train_loss))\n",
        "            #torch.save(model, tag+'/%04i.pth' % config)\n",
        "            break \n",
        "        torch.save(model, 'decoder.pth')\n",
        "#np.savez(tag+'/metrics.npz', MSE=mdl_MSE, KLD=mdl_KLD, MMD=mdl_MMD)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "config 0, dropout = 0.00; 2 hidden layers with 508, 31 nodes\n",
            "====> Epoch: 1 TRAINING Loss: 1.57e+02\n",
            "====> Epoch: 2 TRAINING Loss: 5.99e+01\n",
            "====> Epoch: 3 TRAINING Loss: 4.13e+01\n",
            "====> Epoch: 4 TRAINING Loss: 3.08e+01\n",
            "====> Epoch: 5 TRAINING Loss: 2.58e+01\n",
            "====> Epoch: 6 TRAINING Loss: 2.33e+01\n",
            "====> Epoch: 7 TRAINING Loss: 2.08e+01\n",
            "====> Epoch: 8 TRAINING Loss: 1.97e+01\n",
            "====> Epoch: 9 TRAINING Loss: 1.80e+01\n",
            "====> Epoch: 10 TRAINING Loss: 1.71e+01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBXLhxGtOsb8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}